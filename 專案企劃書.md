# 專案企劃書：互動式強化學習平台 (MVP 版本)

---

## 1. 專案摘要

本專案旨在開發一個基於 Python 的強化學習實踐平台。專案初期 (MVP) 將聚焦於建立一個穩定、可運作的核心系統，讓使用者能夠在一個網格世界 (Grid World) 環境中，運行並紀錄 **Q-Learning** 與 **SARSA** 兩種經典演算法的學習過程。前端展示將以簡潔實用為原則，主要用於呈現 AI 的決策路徑與學習成果的數據分析，確保核心功能的健壯性與數據的完整性。

---

## 2. 專案目標

- **目標一：建立核心環境** - 實現一個功能完整的網格世界遊戲環境，包含狀態、動作、獎勵機制與邊界邏輯。
- **目標二：實現核心演算法** - 以 Python 程式碼清晰地實現 Q-Learning 與 SARSA 演算法。
- **目標三：確保執行與紀錄** - 建立一個完整的訓練流程，允許使用者設定超參數，並能將訓練結果（Q-Table）與學習過程數據（如每回合獎勵）可靠地存檔。
- **目標四：完成基礎分析與展示** - 開發一個能讀取紀錄檔的分析工具，以靜態圖表等簡單形式展示學習成果，驗證演算法的有效性。

---

## 3. 核心功能模組 (MVP 範疇)

### 模組一：地圖建置 (後端定義)

- **功能：** 快速定義遊戲地圖。
- **實現方式：** 初期**不開發複雜的 GUI 編輯器**。地圖將直接在程式碼中以 NumPy 陣列定義，或從一個簡單的 JSON/TXT 設定檔中讀取。這種方式能讓我們專注於核心演算法的開發。
- **範例 `map.json`：**
  ```json
  {
    "name": "Simple Reward Map",
    "size": [10, 10],
    "start": [0, 0],
    "goal": [9, 9],
    "obstacles": [
        [3, 3], [3, 4], [3, 5],
        [6, 6], [7, 6], [8, 6]
    ],
    "bonuses": {
        "5,5": 20
    },
    "traps": {
        "8,8": -50
    }
  }
  ```

### 模組二：手動遊玩模式 (終端機介面)

- **功能：** 驗證地圖規則與獎勵機制。
- **實現方式：** 在終端機 (Terminal) 中運行。程式會印出當前地圖，使用者透過輸入 `w, a, s, d` 進行移動，程式即時回饋分數與步數。
- **產出：** 遊戲結束後，在終端機印出總分與路徑序列 `[(s, a, r, s'), ...]`，可選存成 log 檔。

### 模組三：AI 訓練與紀錄 (核心模組)

- **功能：** 根據設定，執行強化學習演算法並儲存結果。
- **實現方式：** 此為專案核心，以腳本形式運行。
- **參數設定：** 透過程式碼頂部的變數或命令列參數來設定：
    - `ALGORITHM`: `'q-learning'` 或 `'sarsa'`
    - `LEARNING_RATE` (α)
    - `DISCOUNT_FACTOR` (γ)
    - `EPSILON` (ε) (可設定為固定值或簡單的線性衰減)
    - `EPISODES` (訓練回合數)
    - `MAP_FILE` (要載入的地圖檔)
- **執行：** 程式載入地圖，初始化 Q-Table，並開始訓練。訓練過程中，終端機會顯示進度 (例如 `Episode 1000/10000 completed`)。
- **紀錄：**
    - **Q-Table 存檔：** 訓練結束後，將最終的 Q-Table 以 `.csv` 或 `.pkl` 格式儲存，檔名應包含地圖名、演算法與日期，例如 `map1_qlearning_20250721.csv`。
    - **學習日誌：** 同時儲存一個記錄檔，包含每回合的總獎勵與總步數，用於後續繪製學習曲線。

### 模組四：學習成果分析 (靜態圖表生成)

- **功能：** 視覺化已儲存的 Q-Table，分析 AI 學習成果。
- **實現方式：** 開發一個獨立的分析腳本 `analyzer.py`。
- **選擇數據：** 腳本接收一個 Q-Table 檔案路徑作為輸入。
- **生成圖表：**
    - **Q-Table 熱力圖 (Heatmap)：** 針對每個狀態，繪製其所有動作的 Q 值熱力圖。
    - **最優路徑圖 (Optimal Path)：** 根據 Q-Table 推導出的最優策略，在地圖上繪製出路徑。
    - **學習曲線 (Learning Curve)：** 讀取學習日誌，繪製獎勵隨回合數變化的曲線圖。
- **展示：** 將生成的圖表直接顯示或儲存為圖片檔案 (如 `.png`)，而非建構複雜的 GUI 報告介面。

---

## 4. 技術棧

- **主要語言：** Python 3.9+
- **GUI 框架：** 初期不使用。簡單展示可選用 `Pygame` (繪製路徑) 或 `Matplotlib` (直接顯示圖表)。
- **核心計算：** `NumPy`
- **數據處理：** `Pandas` (用於處理 Q-Table)
- **檔案格式：** `JSON` (地圖), `CSV` (Q-Table/Logs)
- **圖表繪製：** `Matplotlib`, `Seaborn`

---

## 5. 強化學習設計

- **狀態 (State)：** 網格中的 `(row, col)` 座標。
- **動作 (Action)：** `{0: 上, 1: 下, 2: 左, 3: 右}`
- **獎勵函數 (Reward)：**
    - 到達終點： `+100`
    - 碰到陷阱： `-50`
    - 獲得獎勵： `+20`
    - 撞牆/出界： `-10` (狀態不變)
    - 每走一步： `-1` (鼓勵效率)

---

## 6. MVP 開發時程規劃 (預計 4 週)

- **第一週：核心引擎建構**
    - 完成 Grid World 環境類別 (`GridEnv`)。
    - 實現地圖的檔案讀取功能。
    - 完成 Q-Learning 與 SARSA 的核心演算法邏輯。
    - **目標：** 能夠以純腳本方式，運行演算法並在記憶體中更新 Q-Table。

- **第二週：數據紀錄與參數化**
    - 完成 Q-Table 與學習日誌的存檔/讀檔功能。
    - 整理程式碼，讓超參數可以從外部輕鬆設定。
    - 完成終端機版本的手動遊玩模式。
    - **目標：** 訓練流程完整，所有重要數據都能被可靠地紀錄。

- **第三週：基礎視覺化與分析**
    - 開發 `analyzer.py` 腳本。
    - 實現熱力圖、最優路徑圖與學習曲線的生成。
    - (可選) 使用 Pygame 簡單繪製網格與最優路徑。
    - **目標：** 能夠對已儲存的數據進行有意義的視覺化分析。

- **第四週：整合、測試與文件撰寫**
    - 將所有模組串連成一個流暢的腳本。
    - 針對不同地圖與參數組合進行完整測試。
    - 撰寫 `README.md`，說明如何設定、運行與分析。
    - **目標：** 交付一個穩定、可用的 MVP 版本。

---

## 7. 預期交付成果

- **Python 原始碼：** 包含環境、演算法、訓練與分析的 `.py` 腳本。
- **範例檔案：** 至少 2 個不同複雜度的 `.json` 地圖檔。
- **說明文件 (`README.md`)：** 詳述專案結構、如何安裝依賴套件、如何運行各模組。
- **範例產出：** 一組由範例地圖訓練出的 Q-Table、日誌檔及對應的分析圖表。

---

## 8. 未來擴展方向

在 MVP 成功基礎上，未來可擴展以下功能：

- 開發完整的 GUI 介面，整合所有功能。
- 實現更複雜的演算法，如 DQN。
- 加入隨機性環境（Slippery Mode）。
- 支援更複雜的地圖元素。
# SARSA æ”¹é€²èªªæ˜

## æ¦‚è¿°
æ ¹æ“šç”¨æˆ¶çš„å»ºè­°ï¼Œå° SARSA æ¼”ç®—æ³•é€²è¡Œäº†å…¨é¢çš„æ”¹é€²ï¼Œè§£æ±ºäº†æ½›åœ¨å•é¡Œä¸¦å¢å¼·äº†åŠŸèƒ½ã€‚SARSA ä½œç‚º on-policy æ¼”ç®—æ³•ï¼Œèˆ‡ Q-Learning çš„ off-policy ç‰¹æ€§æœ‰æ‰€ä¸åŒï¼Œæœ¬æ”¹é€²ä¿æŒäº† SARSA çš„æ ¸å¿ƒç‰¹æ€§ã€‚

## ğŸ”§ ä¿®æ­£çš„å•é¡Œ

### 1. éšœç¤™ç‰©çå‹µç„¡æ•ˆ
**å•é¡Œ**ï¼š`get_reward` å‡½æ•¸ç‚ºéšœç¤™ç‰©å®šç¾©äº† -100 çå‹µï¼Œä½†ä»£ç†ç„¡æ³•ç§»å‹•åˆ°éšœç¤™ç‰©ã€‚

**ä¿®æ­£**ï¼š
```python
def get_reward(cell):
    """ç²å–çå‹µå€¼ï¼ˆç§»é™¤éšœç¤™ç‰©çå‹µï¼Œå› ç‚ºä»£ç†ç„¡æ³•åˆ°é”ï¼‰"""
    if cell == GOAL:
        return REWARD_GOAL
    elif cell == REWARD:
        return REWARD_REWARD
    elif cell == TRAP:
        return REWARD_TRAP
    else:
        return REWARD_DEFAULT
    # ç§»é™¤ REWARD_OBSTACLE = -100
```

**å½±éŸ¿**ï¼šæ¶ˆé™¤äº†ç¨‹å¼é‚è¼¯çš„æ··æ·†ï¼Œä½¿çå‹µå‡½æ•¸æ›´ç¬¦åˆå¯¦éš›æƒ…æ³ã€‚

---

### 2. é™·é˜±çµ‚æ­¢é‚è¼¯ä¸ä¸€è‡´
**å•é¡Œ**ï¼šç¨‹å¼åƒ…åœ¨ GOAL æˆ–æœ€å¤§æ­¥æ•¸æ™‚çµ‚æ­¢ï¼ŒTRAP ä¸çµ‚æ­¢ï¼Œèˆ‡ Q-Learning ä¸ä¸€è‡´ã€‚

**ä¿®æ­£**ï¼š
```python
def is_terminal(cell):
    """åˆ¤æ–·æ˜¯å¦ç‚ºçµ‚æ­¢ç‹€æ…‹ï¼ˆç›®æ¨™æˆ–é™·é˜±ï¼‰"""
    return cell in [GOAL, TRAP]

# ä¿®æ­£çµ‚æ­¢æ¢ä»¶ï¼šç›®æ¨™æˆ–é™·é˜±éƒ½çµ‚æ­¢å›åˆ
if is_terminal(cell) or step == MAX_STEPS:
    break
```

**å½±éŸ¿**ï¼šé™·é˜±ç¾åœ¨æœƒæ­£ç¢ºçµ‚æ­¢å›åˆï¼Œèˆ‡ Q-Learning ä¿æŒä¸€è‡´ã€‚

---

### 3. æ¢ç´¢ç‡å›ºå®š
**å•é¡Œ**ï¼šEPSILON = 0.1 å›ºå®šä¸è®Šï¼Œç¼ºä¹è¡°æ¸›æ©Ÿåˆ¶ã€‚

**ä¿®æ­£**ï¼š
```python
# æ¢ç´¢ç‡è¡°æ¸›è¨­å®š
EPSILON_START = 1.0  # åˆå§‹æ¢ç´¢ç‡
EPSILON_END = 0.01   # æœ€çµ‚æ¢ç´¢ç‡
EPSILON_DECAY = 0.995  # æ¢ç´¢ç‡è¡°æ¸›å› å­

def get_epsilon(episode, total_episodes):
    """è¨ˆç®—ç•¶å‰æ¢ç´¢ç‡ï¼ˆæŒ‡æ•¸è¡°æ¸›ï¼‰"""
    if EPSILON_DECAY == 1.0:
        return EPSILON_START
    
    # æŒ‡æ•¸è¡°æ¸›
    decay_rate = (EPSILON_END / EPSILON_START) ** (1 / total_episodes)
    epsilon = EPSILON_START * (decay_rate ** episode)
    return max(epsilon, EPSILON_END)
```

**å½±éŸ¿**ï¼šæ¢ç´¢ç‡å¾ 1.0 é€æ¼¸è¡°æ¸›åˆ° 0.01ï¼Œæé«˜å­¸ç¿’æ•ˆç‡ã€‚

---

### 4. éŒ¯èª¤è™•ç†ä¸è¶³
**å•é¡Œ**ï¼šç¼ºä¹åœ°åœ–é©—è­‰å’Œæª”æ¡ˆå¯«å…¥éŒ¯èª¤è™•ç†ã€‚

**ä¿®æ­£**ï¼š
```python
def load_map(path):
    """è¼‰å…¥åœ°åœ–æª”æ¡ˆ"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data['map']
    except FileNotFoundError:
        raise ValueError(f'åœ°åœ–æª”æ¡ˆä¸å­˜åœ¨: {path}')
    except json.JSONDecodeError:
        raise ValueError(f'åœ°åœ–æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {path}')
    except KeyError:
        raise ValueError(f'åœ°åœ–æª”æ¡ˆç¼ºå°‘ "map" æ¬„ä½: {path}')

def validate_map(map_grid):
    """é©—è­‰åœ°åœ–çš„æœ‰æ•ˆæ€§"""
    has_start = False
    has_goal = False
    has_trap = False
    
    for i, row in enumerate(map_grid):
        for j, cell in enumerate(row):
            if cell == START:
                has_start = True
            elif cell == GOAL:
                has_goal = True
            elif cell == TRAP:
                has_trap = True
    
    if not has_start:
        raise ValueError('åœ°åœ–ä¸­æ²’æœ‰èµ·é» (S)')
    if not has_goal:
        raise ValueError('åœ°åœ–ä¸­æ²’æœ‰ç›®æ¨™ (G)')
    
    print(f"åœ°åœ–é©—è­‰é€šéï¼šèµ·é» âœ“, ç›®æ¨™ âœ“, é™·é˜± {'âœ“' if has_trap else 'âœ—'}")
    return True

def save_results(qtable_rows, log_records, output_dir):
    """å„²å­˜è¨“ç·´çµæœ"""
    try:
        os.makedirs(output_dir, exist_ok=True)
        # ... å„²å­˜é‚è¼¯
    except PermissionError:
        raise ValueError(f'æ²’æœ‰æ¬Šé™å¯«å…¥ç›®éŒ„: {output_dir}')
    except Exception as e:
        raise ValueError(f'å„²å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
```

**å½±éŸ¿**ï¼šæä¾›å®Œæ•´çš„éŒ¯èª¤è™•ç†ï¼Œæé«˜ç¨‹å¼ç©©å®šæ€§ã€‚

---

### 5. éš¨æ©Ÿæ€§ä¸å¯é‡ç¾
**å•é¡Œ**ï¼šæœªè¨­å®šéš¨æ©Ÿç¨®å­ï¼Œçµæœä¸å¯é‡ç¾ã€‚

**ä¿®æ­£**ï¼š
```python
def main(map_path, episodes, learning_rate, discount_factor, epsilon_start, output_dir, seed=None):
    # è¨­å®šéš¨æ©Ÿç¨®å­ä»¥æé«˜å¯é‡ç¾æ€§
    if seed is not None:
        np.random.seed(seed)
        print(f"éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")
```

**å½±éŸ¿**ï¼šè¨­å®šéš¨æ©Ÿç¨®å­å¾Œï¼Œç›¸åŒåƒæ•¸çš„è¨“ç·´çµæœå¯é‡ç¾ã€‚

---

### 6. Q-Table åˆå§‹åŒ–å–®ä¸€
**å•é¡Œ**ï¼šQ-Table åˆå§‹åŒ–ç‚º 0.0ï¼Œç¼ºä¹å…¶ä»–é¸é …ã€‚

**ä¿®æ­£**ï¼š
```python
# Q-Table åˆå§‹åŒ–è¨­å®š
OPTIMISTIC_INIT = False  # æ˜¯å¦ä½¿ç”¨æ¨‚è§€åˆå§‹åŒ–
OPTIMISTIC_VALUE = 1.0   # æ¨‚è§€åˆå§‹å€¼

# åˆå§‹åŒ– Q-Table
initial_value = OPTIMISTIC_VALUE if OPTIMISTIC_INIT else 0.0
for i in range(rows):
    for j in range(cols):
        if map_grid[i][j] != OBSTACLE:
            for action in get_valid_actions(map_grid, (i, j)):
                q_table[(i, j, action)] = initial_value
```

**å½±éŸ¿**ï¼šæä¾›æ¨‚è§€åˆå§‹åŒ–é¸é …ï¼Œå¯èƒ½åŠ é€Ÿæ”¶æ–‚ã€‚

---

## ğŸš€ æ–°å¢åŠŸèƒ½

### 1. è¨“ç·´é€²åº¦ç›£æ§
```python
# æ¯ 50 å›åˆé¡¯ç¤ºé€²åº¦
if episode % 50 == 0:
    avg_reward = np.mean(episode_rewards[-50:])
    print(f"å›åˆ {episode}/{episodes}, å¹³å‡çå‹µ: {avg_reward:.2f}, æ¢ç´¢ç‡: {current_epsilon:.3f}")
```

### 2. è¨“ç·´çµ±è¨ˆè¼¸å‡º
```python
# è¼¸å‡ºè¨“ç·´çµ±è¨ˆ
final_avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
print(f"\nSARSA è¨“ç·´å®Œæˆï¼")
print(f"æœ€çµ‚ 100 å›åˆå¹³å‡çå‹µ: {final_avg_reward:.2f}")
print(f"ç¸½å›åˆæ•¸: {len(episode_rewards)}")
```

### 3. å¢å¼·çš„è¨˜éŒ„åŠŸèƒ½
```python
log_records.append({
    'episode': episode,
    'step': step,
    'state': state_to_str(state),
    'action': action,
    'reward': reward,
    'next_state': state_to_str(next_pos),
    'done': is_terminal(cell),
    'epsilon': current_epsilon  # æ–°å¢æ¢ç´¢ç‡è¨˜éŒ„
})
```

### 4. æ–°çš„å‘½ä»¤åˆ—åƒæ•¸
```python
parser.add_argument('--seed', type=int, default=None, help='éš¨æ©Ÿç¨®å­ï¼ˆç”¨æ–¼å¯é‡ç¾æ€§ï¼‰')
parser.add_argument('--optimistic', action='store_true', help='ä½¿ç”¨æ¨‚è§€åˆå§‹åŒ–')
```

---

## ğŸ”„ SARSA vs Q-Learning æ ¸å¿ƒå·®ç•°

### 1. ç­–ç•¥é¡å‹
- **SARSA**ï¼šOn-policyï¼ˆåœ¨ç­–ç•¥ä¸Šï¼‰
- **Q-Learning**ï¼šOff-policyï¼ˆé›¢ç­–ç•¥ï¼‰

### 2. æ›´æ–°æ–¹å¼
**SARSA æ›´æ–°å…¬å¼**ï¼š
```python
# SARSA æ›´æ–°å…¬å¼
q_key = (state[0], state[1], action)
q_table[q_key] = q_table.get(q_key, 0.0) + learning_rate * (reward + discount_factor * next_q - q_table.get(q_key, 0.0))
```

**Q-Learning æ›´æ–°å…¬å¼**ï¼š
```python
# Q-Learning æ›´æ–°å…¬å¼
next_qs = [q_table.get((next_pos[0], next_pos[1], a), 0.0) for a in next_valid_actions]
max_next_q = max(next_qs) if next_qs else 0.0
q_table[q_key] = q_table.get(q_key, 0.0) + learning_rate * (reward + discount_factor * max_next_q - q_table.get(q_key, 0.0))
```

### 3. å‹•ä½œé¸æ“‡
**SARSA**ï¼š
- éœ€è¦å…ˆé¸æ“‡ä¸‹ä¸€å€‹å‹•ä½œ
- ä½¿ç”¨å¯¦éš›é¸æ“‡çš„å‹•ä½œçš„ Q å€¼æ›´æ–°
- æ›´ä¿å®ˆï¼Œé¿å…å±éšªå‹•ä½œ

**Q-Learning**ï¼š
- ä½¿ç”¨æ‰€æœ‰å¯èƒ½å‹•ä½œçš„æœ€å¤§ Q å€¼æ›´æ–°
- æ›´æ¿€é€²ï¼Œè¿½æ±‚æœ€å„ªç­–ç•¥

### 4. å¯¦ä½œå·®ç•°
```python
# SARSAï¼šéœ€è¦é¸æ“‡ä¸‹ä¸€å‹•ä½œ
if next_valid_actions:
    if np.random.rand() < current_epsilon:
        next_action = np.random.choice(next_valid_actions)
    else:
        next_q_vals = [q_table.get((next_pos[0], next_pos[1], a), -np.inf) for a in next_valid_actions]
        max_next_q = np.max(next_q_vals)
        best_next_actions = [a for a, q in zip(next_valid_actions, next_q_vals) if q == max_next_q]
        next_action = np.random.choice(best_next_actions)
    next_q = q_table.get((next_pos[0], next_pos[1], next_action), 0.0)
else:
    next_action = None
    next_q = 0.0

# Q-Learningï¼šç›´æ¥ä½¿ç”¨æœ€å¤§å€¼
next_qs = [q_table.get((next_pos[0], next_pos[1], a), 0.0) for a in next_valid_actions]
max_next_q = max(next_qs) if next_qs else 0.0
```

---

## ğŸ“Š æ”¹é€²æ•ˆæœ

### 1. å­¸ç¿’æ•ˆç‡æå‡
- **æ¢ç´¢ç‡è¡°æ¸›**ï¼šå¾é«˜æ¢ç´¢é–‹å§‹ï¼Œé€æ¼¸è½‰å‘åˆ©ç”¨
- **æ¨‚è§€åˆå§‹åŒ–**ï¼šå¯é¸çš„æ¨‚è§€åˆå§‹åŒ–å¯èƒ½åŠ é€Ÿæ”¶æ–‚

### 2. ç©©å®šæ€§å¢å¼·
- **åœ°åœ–é©—è­‰**ï¼šé¿å…ç„¡æ•ˆåœ°åœ–å°è‡´çš„è¨“ç·´å¤±æ•—
- **éš¨æ©Ÿç¨®å­**ï¼šç¢ºä¿çµæœå¯é‡ç¾ï¼Œä¾¿æ–¼å¯¦é©—æ¯”è¼ƒ
- **éŒ¯èª¤è™•ç†**ï¼šå®Œæ•´çš„ç•°å¸¸è™•ç†æ©Ÿåˆ¶

### 3. ç›£æ§èƒ½åŠ›
- **é€²åº¦é¡¯ç¤º**ï¼šå³æ™‚ç›£æ§è¨“ç·´é€²åº¦å’Œæ•ˆæœ
- **çµ±è¨ˆè¼¸å‡º**ï¼šæä¾›è©³ç´°çš„è¨“ç·´çµæœçµ±è¨ˆ

### 4. é‚è¼¯ä¸€è‡´æ€§
- **çµ‚æ­¢æ¢ä»¶**ï¼šé™·é˜±æ­£ç¢ºçµ‚æ­¢å›åˆ
- **çå‹µå‡½æ•¸**ï¼šç§»é™¤ç„¡ç”¨çš„éšœç¤™ç‰©çå‹µ

---

## ğŸ¯ ä½¿ç”¨å»ºè­°

### 1. æ¢ç´¢ç‡è¨­å®š
```bash
# ä½¿ç”¨é è¨­è¡°æ¸›ï¼ˆæ¨è–¦ï¼‰
python sarsa.py --episodes 1000

# è‡ªè¨‚åˆå§‹æ¢ç´¢ç‡
python sarsa.py --episodes 1000 --epsilon 0.5
```

### 2. å¯é‡ç¾å¯¦é©—
```bash
# è¨­å®šéš¨æ©Ÿç¨®å­
python sarsa.py --episodes 500 --seed 42
```

### 3. æ¨‚è§€åˆå§‹åŒ–
```bash
# ä½¿ç”¨æ¨‚è§€åˆå§‹åŒ–
python sarsa.py --episodes 500 --optimistic
```

### 4. å®Œæ•´åƒæ•¸è¨­å®š
```bash
python sarsa.py \
    --map maps/example_map.json \
    --episodes 1000 \
    --learning_rate 0.1 \
    --discount_factor 0.95 \
    --epsilon 1.0 \
    --seed 42 \
    --optimistic \
    --output output
```

---

## ğŸ” SARSA é©ç”¨å ´æ™¯

### 1. å®‰å…¨æ€§è¦æ±‚é«˜
- é¿å…å±éšªå‹•ä½œ
- ä¿å®ˆçš„ç­–ç•¥å­¸ç¿’

### 2. æ¢ç´¢æˆæœ¬é«˜
- éŒ¯èª¤å‹•ä½œä»£åƒ¹æ˜‚è²´
- éœ€è¦ç©©å®šçš„å­¸ç¿’éç¨‹

### 3. ç­–ç•¥ä¸€è‡´æ€§
- å­¸ç¿’ç­–ç•¥èˆ‡åŸ·è¡Œç­–ç•¥ç›¸åŒ
- é¿å…ç­–ç•¥ä¸åŒ¹é…å•é¡Œ

---

## ğŸ” é€²ä¸€æ­¥æ”¹é€²å»ºè­°

### 1. SARSA(Î»)
- å¯¦ä½œè³‡æ ¼è·¡ (eligibility traces)
- æé«˜å­¸ç¿’æ•ˆç‡

### 2. æœŸæœ› SARSA
- ä½¿ç”¨æœŸæœ›å€¼è€Œéå¯¦éš›é¸æ“‡çš„å‹•ä½œ
- å¹³è¡¡ SARSA å’Œ Q-Learning

### 3. é›™é‡ SARSA
- ä½¿ç”¨å…©å€‹ Q-Table
- æ¸›å°‘éåº¦ä¼°è¨ˆå•é¡Œ

### 4. å„ªå…ˆç´šç¶“é©—å›æ”¾
- æ ¹æ“š TD èª¤å·®è¨­å®šå„ªå…ˆç´š
- æé«˜é‡è¦ç¶“é©—çš„å­¸ç¿’æ•ˆç‡

### 5. è‡ªé©æ‡‰å­¸ç¿’ç‡
- æ ¹æ“šç‹€æ…‹è¨ªå•æ¬¡æ•¸èª¿æ•´å­¸ç¿’ç‡
- æé«˜å­¸ç¿’ç©©å®šæ€§

é€™äº›æ”¹é€²ä½¿ SARSA æ¼”ç®—æ³•æ›´åŠ ç©©å®šã€é«˜æ•ˆä¸”æ˜“æ–¼ä½¿ç”¨ï¼ŒåŒæ™‚ä¿æŒäº†å…¶ on-policy çš„æ ¸å¿ƒç‰¹æ€§ï¼Œç‚ºé€²ä¸€æ­¥çš„å¼·åŒ–å­¸ç¿’ç ”ç©¶å¥ å®šäº†è‰¯å¥½åŸºç¤ã€‚ 
# Q-Learning æ”¹é€²èªªæ˜

## æ¦‚è¿°
æ ¹æ“šç”¨æˆ¶çš„å»ºè­°ï¼Œå° Q-Learning æ¼”ç®—æ³•é€²è¡Œäº†å…¨é¢çš„æ”¹é€²ï¼Œè§£æ±ºäº†æ½›åœ¨å•é¡Œä¸¦å¢å¼·äº†åŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯è©³ç´°çš„æ”¹é€²å…§å®¹ã€‚

## ğŸ”§ ä¿®æ­£çš„å•é¡Œ

### 1. éšœç¤™ç‰©çå‹µæ··æ·†
**å•é¡Œ**ï¼š`get_reward` å‡½æ•¸ç‚ºéšœç¤™ç‰©å®šç¾©äº† -100 çå‹µï¼Œä½†ä»£ç†ç„¡æ³•ç§»å‹•åˆ°éšœç¤™ç‰©ã€‚

**ä¿®æ­£**ï¼š
```python
def get_reward(cell):
    """ç²å–çå‹µå€¼ï¼ˆç§»é™¤éšœç¤™ç‰©çå‹µï¼Œå› ç‚ºä»£ç†ç„¡æ³•åˆ°é”ï¼‰"""
    if cell == GOAL:
        return REWARD_GOAL
    elif cell == REWARD:
        return REWARD_REWARD
    elif cell == TRAP:
        return REWARD_TRAP
    else:
        return REWARD_DEFAULT
    # ç§»é™¤ REWARD_OBSTACLE = -100
```

**å½±éŸ¿**ï¼šæ¶ˆé™¤äº†ç¨‹å¼é‚è¼¯çš„æ··æ·†ï¼Œä½¿çå‹µå‡½æ•¸æ›´ç¬¦åˆå¯¦éš›æƒ…æ³ã€‚

---

### 2. çµ‚æ­¢æ¢ä»¶ä¸ä¸€è‡´
**å•é¡Œ**ï¼š`is_terminal` å‡½æ•¸å®šç¾©äº† GOAL å’Œ TRAP ç‚ºçµ‚æ­¢ç‹€æ…‹ï¼Œä½†å¯¦éš›åªåœ¨ GOAL æ™‚çµ‚æ­¢ã€‚

**ä¿®æ­£**ï¼š
```python
def is_terminal(cell):
    """åˆ¤æ–·æ˜¯å¦ç‚ºçµ‚æ­¢ç‹€æ…‹ï¼ˆç›®æ¨™æˆ–é™·é˜±ï¼‰"""
    return cell in [GOAL, TRAP]

# ä¿®æ­£çµ‚æ­¢æ¢ä»¶ï¼šç›®æ¨™æˆ–é™·é˜±éƒ½çµ‚æ­¢å›åˆ
if is_terminal(cell) or step == MAX_STEPS:
    break
```

**å½±éŸ¿**ï¼šé™·é˜±ç¾åœ¨æœƒæ­£ç¢ºçµ‚æ­¢å›åˆï¼Œç¬¦åˆè¨­è¨ˆæ„åœ–ã€‚

---

### 3. æ¢ç´¢ç‡å›ºå®š
**å•é¡Œ**ï¼šEPSILON å›ºå®šç‚º 0.1ï¼Œç¼ºä¹è¡°æ¸›æ©Ÿåˆ¶ã€‚

**ä¿®æ­£**ï¼š
```python
# æ¢ç´¢ç‡è¡°æ¸›è¨­å®š
EPSILON_START = 1.0  # åˆå§‹æ¢ç´¢ç‡
EPSILON_END = 0.01   # æœ€çµ‚æ¢ç´¢ç‡
EPSILON_DECAY = 0.995  # æ¢ç´¢ç‡è¡°æ¸›å› å­

def get_epsilon(episode, total_episodes):
    """è¨ˆç®—ç•¶å‰æ¢ç´¢ç‡ï¼ˆæŒ‡æ•¸è¡°æ¸›ï¼‰"""
    if EPSILON_DECAY == 1.0:
        return EPSILON_START
    
    # æŒ‡æ•¸è¡°æ¸›
    decay_rate = (EPSILON_END / EPSILON_START) ** (1 / total_episodes)
    epsilon = EPSILON_START * (decay_rate ** episode)
    return max(epsilon, EPSILON_END)
```

**å½±éŸ¿**ï¼šæ¢ç´¢ç‡å¾ 1.0 é€æ¼¸è¡°æ¸›åˆ° 0.01ï¼Œæé«˜å­¸ç¿’æ•ˆç‡ã€‚

---

### 4. åœ°åœ–é©—è­‰ä¸è¶³
**å•é¡Œ**ï¼šç¨‹å¼æœªè™•ç†åœ°åœ–ç„¡æ•ˆçš„æƒ…æ³ï¼ˆå¦‚ç„¡ç›®æ¨™ Gï¼‰ã€‚

**ä¿®æ­£**ï¼š
```python
def validate_map(map_grid):
    """é©—è­‰åœ°åœ–çš„æœ‰æ•ˆæ€§"""
    has_start = False
    has_goal = False
    has_trap = False
    
    for i, row in enumerate(map_grid):
        for j, cell in enumerate(row):
            if cell == START:
                has_start = True
            elif cell == GOAL:
                has_goal = True
            elif cell == TRAP:
                has_trap = True
    
    if not has_start:
        raise ValueError('åœ°åœ–ä¸­æ²’æœ‰èµ·é» (S)')
    if not has_goal:
        raise ValueError('åœ°åœ–ä¸­æ²’æœ‰ç›®æ¨™ (G)')
    
    print(f"åœ°åœ–é©—è­‰é€šéï¼šèµ·é» âœ“, ç›®æ¨™ âœ“, é™·é˜± {'âœ“' if has_trap else 'âœ—'}")
    return True
```

**å½±éŸ¿**ï¼šåœ¨è¨“ç·´é–‹å§‹å‰é©—è­‰åœ°åœ–æœ‰æ•ˆæ€§ï¼Œé¿å…ç„¡æ„ç¾©çš„è¨“ç·´ã€‚

---

### 5. éš¨æ©Ÿæ€§ä¸å¯é‡ç¾
**å•é¡Œ**ï¼šæœªè¨­å®šéš¨æ©Ÿç¨®å­ï¼Œçµæœä¸å¯é‡ç¾ã€‚

**ä¿®æ­£**ï¼š
```python
def main(map_path, episodes, learning_rate, discount_factor, epsilon_start, output_dir, seed=None):
    # è¨­å®šéš¨æ©Ÿç¨®å­ä»¥æé«˜å¯é‡ç¾æ€§
    if seed is not None:
        np.random.seed(seed)
        print(f"éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")
```

**å½±éŸ¿**ï¼šè¨­å®šéš¨æ©Ÿç¨®å­å¾Œï¼Œç›¸åŒåƒæ•¸çš„è¨“ç·´çµæœå¯é‡ç¾ã€‚

---

### 6. Q-Table åˆå§‹åŒ–å–®ä¸€
**å•é¡Œ**ï¼šQ-Table åˆå§‹åŒ–ç‚º 0.0ï¼Œç¼ºä¹å…¶ä»–é¸é …ã€‚

**ä¿®æ­£**ï¼š
```python
# Q-Table åˆå§‹åŒ–è¨­å®š
OPTIMISTIC_INIT = False  # æ˜¯å¦ä½¿ç”¨æ¨‚è§€åˆå§‹åŒ–
OPTIMISTIC_VALUE = 1.0   # æ¨‚è§€åˆå§‹å€¼

# åˆå§‹åŒ– Q-Table
initial_value = OPTIMISTIC_VALUE if OPTIMISTIC_INIT else 0.0
for i in range(rows):
    for j in range(cols):
        if map_grid[i][j] != OBSTACLE:
            for action in get_valid_actions(map_grid, (i, j)):
                q_table[(i, j, action)] = initial_value
```

**å½±éŸ¿**ï¼šæä¾›æ¨‚è§€åˆå§‹åŒ–é¸é …ï¼Œå¯èƒ½åŠ é€Ÿæ”¶æ–‚ã€‚

---

## ğŸš€ æ–°å¢åŠŸèƒ½

### 1. è¨“ç·´é€²åº¦ç›£æ§
```python
# æ¯ 50 å›åˆé¡¯ç¤ºé€²åº¦
if episode % 50 == 0:
    avg_reward = np.mean(episode_rewards[-50:])
    print(f"å›åˆ {episode}/{episodes}, å¹³å‡çå‹µ: {avg_reward:.2f}, æ¢ç´¢ç‡: {current_epsilon:.3f}")
```

### 2. è¨“ç·´çµ±è¨ˆè¼¸å‡º
```python
# è¼¸å‡ºè¨“ç·´çµ±è¨ˆ
final_avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
print(f"\nè¨“ç·´å®Œæˆï¼")
print(f"æœ€çµ‚ 100 å›åˆå¹³å‡çå‹µ: {final_avg_reward:.2f}")
print(f"ç¸½å›åˆæ•¸: {len(episode_rewards)}")
```

### 3. å¢å¼·çš„è¨˜éŒ„åŠŸèƒ½
```python
log_records.append({
    'episode': episode,
    'step': step,
    'state': state_to_str(state),
    'action': action,
    'reward': reward,
    'next_state': state_to_str(next_pos),
    'done': is_terminal(cell),
    'epsilon': current_epsilon  # æ–°å¢æ¢ç´¢ç‡è¨˜éŒ„
})
```

### 4. æ–°çš„å‘½ä»¤åˆ—åƒæ•¸
```python
parser.add_argument('--seed', type=int, default=None, help='éš¨æ©Ÿç¨®å­ï¼ˆç”¨æ–¼å¯é‡ç¾æ€§ï¼‰')
parser.add_argument('--optimistic', action='store_true', help='ä½¿ç”¨æ¨‚è§€åˆå§‹åŒ–')
```

---

## ğŸ“Š æ”¹é€²æ•ˆæœ

### 1. å­¸ç¿’æ•ˆç‡æå‡
- **æ¢ç´¢ç‡è¡°æ¸›**ï¼šå¾é«˜æ¢ç´¢é–‹å§‹ï¼Œé€æ¼¸è½‰å‘åˆ©ç”¨ï¼Œæé«˜å­¸ç¿’æ•ˆç‡
- **æ¨‚è§€åˆå§‹åŒ–**ï¼šå¯é¸çš„æ¨‚è§€åˆå§‹åŒ–å¯èƒ½åŠ é€Ÿæ”¶æ–‚

### 2. ç©©å®šæ€§å¢å¼·
- **åœ°åœ–é©—è­‰**ï¼šé¿å…ç„¡æ•ˆåœ°åœ–å°è‡´çš„è¨“ç·´å¤±æ•—
- **éš¨æ©Ÿç¨®å­**ï¼šç¢ºä¿çµæœå¯é‡ç¾ï¼Œä¾¿æ–¼å¯¦é©—æ¯”è¼ƒ

### 3. ç›£æ§èƒ½åŠ›
- **é€²åº¦é¡¯ç¤º**ï¼šå³æ™‚ç›£æ§è¨“ç·´é€²åº¦å’Œæ•ˆæœ
- **çµ±è¨ˆè¼¸å‡º**ï¼šæä¾›è©³ç´°çš„è¨“ç·´çµæœçµ±è¨ˆ

### 4. é‚è¼¯ä¸€è‡´æ€§
- **çµ‚æ­¢æ¢ä»¶**ï¼šé™·é˜±æ­£ç¢ºçµ‚æ­¢å›åˆ
- **çå‹µå‡½æ•¸**ï¼šç§»é™¤ç„¡ç”¨çš„éšœç¤™ç‰©çå‹µ

---

## ğŸ¯ ä½¿ç”¨å»ºè­°

### 1. æ¢ç´¢ç‡è¨­å®š
```bash
# ä½¿ç”¨é è¨­è¡°æ¸›ï¼ˆæ¨è–¦ï¼‰
python q_learning.py --episodes 1000

# è‡ªè¨‚åˆå§‹æ¢ç´¢ç‡
python q_learning.py --episodes 1000 --epsilon 0.5
```

### 2. å¯é‡ç¾å¯¦é©—
```bash
# è¨­å®šéš¨æ©Ÿç¨®å­
python q_learning.py --episodes 500 --seed 42
```

### 3. æ¨‚è§€åˆå§‹åŒ–
```bash
# ä½¿ç”¨æ¨‚è§€åˆå§‹åŒ–
python q_learning.py --episodes 500 --optimistic
```

### 4. å®Œæ•´åƒæ•¸è¨­å®š
```bash
python q_learning.py \
    --map maps/example_map.json \
    --episodes 1000 \
    --learning_rate 0.1 \
    --discount_factor 0.95 \
    --epsilon 1.0 \
    --seed 42 \
    --optimistic \
    --output output
```

---

## ğŸ” é€²ä¸€æ­¥æ”¹é€²å»ºè­°

### 1. è‡ªé©æ‡‰å­¸ç¿’ç‡
- æ ¹æ“šè¨“ç·´é€²åº¦å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡
- ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦å™¨

### 2. ç¶“é©—å›æ”¾
- å¯¦ä½œç¶“é©—å›æ”¾æ©Ÿåˆ¶
- æé«˜æ¨£æœ¬æ•ˆç‡

### 3. ç›®æ¨™ç¶²è·¯
- ä½¿ç”¨ç›®æ¨™ç¶²è·¯æé«˜ç©©å®šæ€§
- å®šæœŸæ›´æ–°ç›®æ¨™ç¶²è·¯

### 4. å„ªå…ˆç´šç¶“é©—å›æ”¾
- æ ¹æ“š TD èª¤å·®è¨­å®šå„ªå…ˆç´š
- æé«˜é‡è¦ç¶“é©—çš„å­¸ç¿’æ•ˆç‡

### 5. å¤šæ­¥å­¸ç¿’
- ä½¿ç”¨ n-step å›å ±
- å¹³è¡¡åå·®å’Œæ–¹å·®

é€™äº›æ”¹é€²ä½¿ Q-Learning æ¼”ç®—æ³•æ›´åŠ ç©©å®šã€é«˜æ•ˆä¸”æ˜“æ–¼ä½¿ç”¨ï¼Œç‚ºé€²ä¸€æ­¥çš„å¼·åŒ–å­¸ç¿’ç ”ç©¶å¥ å®šäº†è‰¯å¥½åŸºç¤ã€‚ 
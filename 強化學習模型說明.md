# 強化學習模型說明

## 概述
本專案實作了兩種經典的強化學習演算法：**Q-Learning** 和 **SARSA**，用於解決 Grid World 環境中的路徑規劃問題。

## 核心演算法

### 1. Q-Learning（離線型強化學習）

#### 理論基礎
Q-Learning 是一種 off-policy（離策略）的強化學習演算法，它學習的是最優動作價值函數 Q*(s,a)，而不依賴於當前執行的策略。

#### 核心公式
```
Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
```

其中：
- **Q(s,a)**：狀態 s 下執行動作 a 的價值
- **α**：學習率（learning rate）
- **r**：即時獎勵
- **γ**：折扣因子（discount factor）
- **s'**：下一個狀態
- **a'**：下一個動作

#### 實作特點
- 使用 ε-greedy 策略平衡探索與利用
- 直接學習最優策略，不受當前策略影響
- 更新時使用所有可能動作的最大 Q 值

### 2. SARSA（在線型強化學習）

#### 理論基礎
SARSA 是一種 on-policy（在策略）的強化學習演算法，它學習的是當前策略下的動作價值函數 Qπ(s,a)。

#### 核心公式
```
Q(s,a) ← Q(s,a) + α[r + γ Q(s',a') - Q(s,a)]
```

#### 實作特點
- 需要先選擇下一個動作 a'
- 使用實際選擇的動作的 Q 值進行更新
- 更保守，避免危險動作

## 關鍵參數

### 1. 學習率 (α)
- **作用**：控制新知識覆蓋舊知識的速度
- **範圍**：0 ~ 1
- **預設值**：0.1
- **建議值**：0.1 ~ 0.5

### 2. 折扣因子 (γ)
- **作用**：平衡即時獎勵與未來獎勵的重要性
- **範圍**：0 ~ 1
- **預設值**：0.95
- **建議值**：0.8 ~ 0.99

### 3. 探索率 (ε)
- **作用**：控制隨機探索的機率
- **範圍**：0 ~ 1
- **預設值**：1.0（初始值）
- **衰減機制**：指數衰減到 0.01
- **建議值**：0.5 ~ 1.0（初始值）

## 環境設定

### Grid World 元素
- **S**：起點（Start）
- **G**：目標（Goal）
- **R**：獎勵點（Reward）
- **T**：陷阱（Trap）
- **1**：障礙物（Obstacle）
- **0**：空地（Empty）

### 獎勵設計
- **目標 (G)**：+100
- **獎勵點 (R)**：+10
- **陷阱 (T)**：-50
- **空地 (0)**：-1
- **障礙物 (1)**：無法進入

### 終止條件
- 到達目標 (G)
- 進入陷阱 (T)
- 達到最大步數

## 演算法比較

| 特性 | Q-Learning | SARSA |
|------|------------|-------|
| 策略類型 | Off-policy | On-policy |
| 更新方式 | 使用最大 Q 值 | 使用實際選擇的 Q 值 |
| 探索性 | 更積極 | 更保守 |
| 收斂速度 | 較快 | 較慢 |
| 穩定性 | 較低 | 較高 |
| 適用場景 | 追求最優解 | 安全性要求高 |

## 輸出結果

### 1. Q-Table
- 格式：CSV 檔案
- 內容：每個狀態-動作對的 Q 值
- 用途：分析學習結果，找出最優路徑

### 2. 訓練記錄
- 格式：CSV 檔案
- 內容：每步的狀態、動作、獎勵、探索率
- 用途：分析學習過程，繪製學習曲線

### 3. 學習曲線
- 格式：PNG 圖片
- 內容：獎勵、步數、探索率隨回合的變化
- 用途：評估學習效果，調整參數

## 優化建議

### 1. 探索率衰減
- 使用指數衰減：ε = ε_start × (decay_rate)^episode
- 從高探索開始，逐漸轉向利用
- 避免過早收斂到局部最優

### 2. 隨機種子設定
- 設定固定的隨機種子
- 確保結果可重現
- 便於實驗比較

### 3. 樂觀初始化
- 將 Q-Table 初始值設為較高值
- 鼓勵探索所有狀態-動作對
- 可能加速收斂

### 4. 地圖驗證
- 檢查地圖是否包含起點和目標
- 確保地圖格式正確
- 避免訓練失敗

## 未來擴展

### 1. 進階演算法
- **SARSA(λ)**：使用資格跡提高學習效率
- **期望 SARSA**：平衡 SARSA 和 Q-Learning
- **雙重 Q-Learning**：減少過度估計問題

### 2. 環境擴展
- **連續狀態空間**：使用函數近似
- **多智能體環境**：協作與競爭
- **動態環境**：環境變化適應

### 3. 應用領域
- **機器人導航**：路徑規劃
- **遊戲 AI**：策略學習
- **資源分配**：優化決策

## 使用建議

### 1. 參數調優順序
1. 調整學習率 (α)
2. 調整折扣因子 (γ)
3. 調整探索率衰減
4. 嘗試樂觀初始化

### 2. 演算法選擇
- **一般情況**：優先使用 Q-Learning
- **安全性要求高**：使用 SARSA
- **需要穩定性**：使用 SARSA
- **追求最優解**：使用 Q-Learning

### 3. 實驗設計
- 使用相同隨機種子比較不同參數
- 多次運行取平均值
- 記錄完整的實驗配置
- 分析學習曲線的穩定性 
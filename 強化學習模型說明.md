# 強化學習模型說明

## 概述
本專案實作了兩種經典的強化學習演算法：**Q-Learning** 和 **SARSA**，用於解決 Grid World 環境中的路徑規劃問題。

## 環境設定

### Grid World 環境
- **狀態空間**：每個格子位置 (i, j) 代表一個狀態
- **動作空間**：{上, 下, 左, 右} 四個動作
- **地圖元素**：
  - `S`：起點 (Start)
  - `G`：目標 (Goal)，獎勵 +100
  - `R`：獎勵點 (Reward)，獎勵 +10
  - `T`：陷阱 (Trap)，獎勵 -50
  - `1`：障礙物 (Obstacle)，獎勵 -100
  - `0`：空白格子，獎勵 -1

### 終止條件
- 到達目標 (Goal)
- 達到最大步數 (MAX_STEPS = 100)

## Q-Learning 演算法

### 理論基礎
Q-Learning 是一種 **off-policy** 的強化學習演算法，使用 Q-Table 來儲存狀態-動作價值函數。

### 核心公式
```
Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
```

其中：
- `Q(s,a)`：狀態 s 下執行動作 a 的價值
- `α`：學習率 (learning_rate)
- `r`：即時獎勵
- `γ`：折扣因子 (discount_factor)
- `s'`：下一個狀態
- `a'`：下一個動作

### 實作特點
1. **Off-policy**：使用 ε-greedy 策略選擇動作，但更新時考慮所有可能動作的最大值
2. **探索策略**：ε-greedy 策略
   - 機率 ε：隨機選擇動作
   - 機率 1-ε：選擇 Q 值最大的動作
3. **Q-Table 結構**：`{(i, j, action): Q_value}`

### 關鍵程式碼
```python
# Q-Learning 更新
q_key = (state[0], state[1], action)
next_qs = [q_table.get((next_pos[0], next_pos[1], a), 0.0) for a in next_valid_actions]
max_next_q = max(next_qs) if next_qs else 0.0
q_table[q_key] = q_table.get(q_key, 0.0) + learning_rate * (reward + discount_factor * max_next_q - q_table.get(q_key, 0.0))
```

## SARSA 演算法

### 理論基礎
SARSA 是一種 **on-policy** 的強化學習演算法，名稱來自於 (State, Action, Reward, State, Action)。

### 核心公式
```
Q(s,a) ← Q(s,a) + α[r + γ Q(s',a') - Q(s,a)]
```

其中：
- `Q(s,a)`：狀態 s 下執行動作 a 的價值
- `α`：學習率 (learning_rate)
- `r`：即時獎勵
- `γ`：折扣因子 (discount_factor)
- `s'`：下一個狀態
- `a'`：實際執行的下一個動作

### 實作特點
1. **On-policy**：使用相同的策略選擇動作和更新 Q 值
2. **探索策略**：ε-greedy 策略（與 Q-Learning 相同）
3. **動作選擇**：需要先選擇下一個動作，再更新當前 Q 值

### 關鍵程式碼
```python
# SARSA 更新
q_key = (state[0], state[1], action)
if next_valid_actions:
    if np.random.rand() < epsilon:
        next_action = np.random.choice(next_valid_actions)
    else:
        next_q_vals = [q_table.get((next_pos[0], next_pos[1], a), -np.inf) for a in next_valid_actions]
        max_next_q = np.max(next_q_vals)
        best_next_actions = [a for a, q in zip(next_valid_actions, next_q_vals) if q == max_next_q]
        next_action = np.random.choice(best_next_actions)
    next_q = q_table.get((next_pos[0], next_pos[1], next_action), 0.0)
else:
    next_q = 0.0

q_table[q_key] = q_table.get(q_key, 0.0) + learning_rate * (reward + discount_factor * next_q - q_table.get(q_key, 0.0))
```

## 參數說明

### 學習率 (Learning Rate, α)
- **範圍**：0 < α ≤ 1
- **作用**：控制新資訊對 Q 值更新的影響程度
- **預設值**：0.1
- **影響**：
  - 較大：學習快速但可能不穩定
  - 較小：學習穩定但速度慢

### 折扣因子 (Discount Factor, γ)
- **範圍**：0 ≤ γ < 1
- **作用**：平衡即時獎勵和未來獎勵的重要性
- **預設值**：0.95
- **影響**：
  - 較大：重視長期獎勵
  - 較小：重視短期獎勵

### 探索率 (Epsilon, ε)
- **範圍**：0 ≤ ε ≤ 1
- **作用**：控制探索與利用的平衡
- **預設值**：0.1
- **影響**：
  - 較大：更多探索，學習新策略
  - 較小：更多利用，執行已知最佳策略

### 訓練回合數 (Episodes)
- **預設值**：500
- **作用**：控制訓練的總回合數
- **影響**：回合數越多，學習效果越好，但訓練時間越長

## Q-Learning vs SARSA 比較

| 特性 | Q-Learning | SARSA |
|------|------------|-------|
| **策略類型** | Off-policy | On-policy |
| **更新方式** | 使用最大 Q 值 | 使用實際選擇的動作 |
| **收斂性** | 收斂到最優策略 | 收斂到當前策略的最優值 |
| **穩定性** | 較不穩定 | 較穩定 |
| **適用場景** | 需要最優解 | 需要安全策略 |
| **計算複雜度** | 較低 | 較高 |

## 輸出結果

### Q-Table (q_table.csv)
包含每個狀態-動作對的 Q 值：
```csv
state,action,value
(0,0),up,0.5
(0,0),right,0.8
(0,1),up,0.3
...
```

### 訓練記錄 (log.csv)
記錄每個步驟的詳細資訊：
```csv
episode,step,state,action,reward,next_state,done
1,1,(0,0),right,-1,(0,1),False
1,2,(0,1),down,10,(1,1),False
...
```

## 最佳化建議

1. **參數調優**：
   - 根據環境複雜度調整學習率
   - 根據任務性質調整折扣因子
   - 使用衰減的探索率

2. **獎勵設計**：
   - 確保獎勵函數能引導到目標
   - 避免獎勵稀疏問題
   - 考慮使用獎勵塑形 (Reward Shaping)

3. **訓練策略**：
   - 使用經驗回放 (Experience Replay)
   - 實作目標網路 (Target Network)
   - 考慮使用優先級經驗回放

## 擴展可能性

1. **深度強化學習**：
   - DQN (Deep Q-Network)
   - DDPG (Deep Deterministic Policy Gradient)

2. **多智能體強化學習**：
   - 競爭環境
   - 合作環境

3. **連續動作空間**：
   - Actor-Critic 方法
   - Policy Gradient 方法 
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>強化學習訓練分析報告</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+TC:wght@400;700&display=swap');

        body {
            font-family: 'Noto Sans TC', sans-serif;
            line-height: 1.8;
            background-color: #f8f9fa;
            color: #343a40;
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 960px;
            margin: auto;
            background: #ffffff;
            padding: 2rem;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
        }

        header {
            border-bottom: 2px solid #007bff;
            padding-bottom: 1rem;
            margin-bottom: 2rem;
            text-align: center;
        }

        h1 {
            color: #007bff;
            font-size: 2.5rem;
            margin: 0;
        }

        h2 {
            color: #17a2b8;
            border-left: 5px solid #17a2b8;
            padding-left: 1rem;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
        }

        h3 {
            color: #28a745;
            margin-top: 2rem;
            font-size: 1.4rem;
        }
        
        .summary-block {
            background-color: #e9ecef;
            border: 1px solid #dee2e6;
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 2rem;
        }

        .summary-block p {
            margin: 0.5rem 0;
        }

        .summary-block strong {
            color: #0056b3;
        }

        ul {
            padding-left: 20px;
        }

        li {
            margin-bottom: 0.8rem;
        }

        code {
            background-color: #e8e8e8;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 1.5rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #dee2e6;
            text-align: left;
        }

        th {
            background-color: #007bff;
            color: #ffffff;
        }

        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .score-card {
            background: linear-gradient(135deg, #28a745, #20c997);
            color: white;
            text-align: center;
            padding: 2rem;
            border-radius: 10px;
            margin-top: 2rem;
        }
        
        .score {
            font-size: 4rem;
            font-weight: 700;
        }
        
        .score-text {
            font-size: 1.2rem;
            margin-top: 0;
        }
        
        .warning {
            color: #dc3545;
            font-weight: 700;
        }
        
        .success {
            color: #28a745;
            font-weight: 700;
        }

        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            h1 {
                font-size: 2rem;
            }
            h2 {
                font-size: 1.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>強化學習訓練分析報告</h1>
        </header>

        <main>
            <section id="summary">
                <h2>報告摘要</h2>
                <div class="summary-block">
                    <p><strong>分析顧問:</strong> AI 分析顧問</p>
                    <p><strong>分析日期:</strong> 2023年10月27日</p>
                    <p><strong>核心結論:</strong> 本次訓練展示了代理（Agent）學習到解決問題路徑的<strong>潛力</strong>，但訓練過程<span class="warning">極不穩定</span>且<span class="warning">遠未收斂</span>。代理的表現在成功與災難性失敗之間劇烈波動，表明當前參數和訓練策略存在顯著問題。儘管最終回合表現良好，但這更可能是隨機運氣，而非穩定能力的體現。</p>
                </div>
            </section>

            <section id="evaluation">
                <h2>1. 學習效果評估</h2>

                <h3>1.1 學習曲線分析</h3>
                 <div style="width:100%; margin: 2rem auto;">
                    <canvas id="learningCurveChart"></canvas>
                </div>
                <ul>
                    <li><strong>獎勵趨勢:</strong> 數據摘要中的「獎勵趨勢：下降」具有誤導性。實際數據點顯示出<span class="warning">劇烈震盪</span>，而非單純下降。獎勵值在非常高的正值（如113）和非常低的負值（如-95）之間來回跳動，這是學習不穩定的典型標誌。</li>
                    <li><strong>步數趨勢:</strong> 步數同樣表現出高方差，與獎勵的震盪同步。高效的8步和災難性的46步交替出現。</li>
                    <li><strong>結論:</strong> 學習曲線表明代理處於一個<span class="warning">不穩定的探索階段</span>，無法穩定地複現其最佳策略。</li>
                </ul>

                <h3>1.2 策略學習評估</h3>
                <ul>
                    <li><strong>是否學到有效策略:</strong> <span class="success">部分學到</span>。Q-Table和最優路徑分析顯示，代理已識別出高價值路徑，但執行不穩定。</li>
                    <li><strong>是否收斂:</strong> <span class="warning">完全沒有收斂</span>。收斂的標誌是獎勵曲線趨於平穩高位，當前的劇烈震盪是未收斂的典型特徵。</li>
                    <li><strong>最終性能:</strong> 最終回合的「獎勵104，步數8」是一個孤立的<span class="success">優秀數據點</span>，但不能代表模型的平均穩定性能。</li>
                </ul>
            </section>

            <section id="diagnosis">
                <h2>2. 問題診斷</h2>
                <h3>2.1 核心問題識別</h3>
                <ol>
                    <li><strong>學習不穩定性 (High Variance):</strong> 最主要的問題。策略在不同回合間差異巨大，可能由過高的學習率或不佳的探索/利用平衡導致。</li>
                    <li><strong>探索策略不佳:</strong> 代理似乎在訓練後期仍在進行過度探索，導致頻繁陷入懲罰區域。<code>epsilon</code>衰減策略可能需要調整。</li>
                    <li><strong>訓練回合數嚴重不足:</strong> 100個回合對於RL問題來說太少了，模型沒有足夠的時間來穩定Q值和收斂策略。</li>
                </ol>

                <h3>2.2 Q-Table 與路徑分析</h3>
                <ul>
                    <li><strong>Q-Table質量:</strong> <span class="success">質量合理</span>。價值從目標向起點逐步遞減，表明價值反向傳播有效。這是個積極信號。</li>
                    <li><strong>最優路徑合理性:</strong> 代理選擇的路徑是一條<span class="success">完整且高效的路線</span>，證明Q-Table存儲的知識有價值，問題在於策略執行的穩定性。</li>
                    <li><strong>過擬合/欠擬合:</strong> 更接近於<span class="warning">欠擬合 (Underfitting)</span> 或 **訓練不足 (Under-training)**。</li>
                </ul>
            </section>

            <section id="recommendations">
                <h2>3. 改進建議</h2>
                <h3>3.1 參數調整</h3>
                <table >
                    <thead>
                        <tr>
                            <th>參數</th>
                            <th>建議</th>
                            <th>理由</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>探索率 (Epsilon, ε)</strong></td>
                            <td>採用更積極的衰減策略（如在前20%回合快速衰減，後續慢速衰減）。</td>
                            <td>避免在訓練後期過度探索，促使模型利用已學知識。</td>
                        </tr>
                        <tr>
                            <td><strong>學習率 (Alpha, α)</strong></td>
                            <td><strong>顯著降低學習率</strong>（如從0.1降至0.01或更低）。</td>
                            <td>過高的學習率是導致學習震盪和不收斂的主要原因。</td>
                        </tr>
                        <tr>
                            <td><strong>折扣因子 (Gamma, γ)</strong></td>
                            <td>保持在0.9-0.99，作為次要調整選項。</td>
                            <td>當前Q-Table顯示價值傳播似乎正常，優先調整前兩者。</td>
                        </tr>
                    </tbody>
                </table>

                <h3>3.2 訓練策略優化</h3>
                 <ul>
                    <li><strong>增加訓練回合數:</strong> 將總回合數從100次<span class="success">大幅增加到至少2000次以上</span>，並持續監控學習曲線。這是最關鍵的改進之一。</li>
                    <li><strong>獎勵塑形 (Reward Shaping):</strong> 考慮將極端的負獎勵（如-95）調整為更溫和的懲罰（如-20），以實現更平滑的學習過程。</li>
                 </ul>
            </section>

            <section id="algorithm-analysis">
                <h2>4. 算法特性分析</h2>
                <ul>
                    <li><strong>推測算法:</strong> 很可能是 <strong>Q-Learning</strong> 或 <strong>SARSA</strong>，一種表格型RL算法。</li>
                    <li><strong>優點:</strong> 可解釋性強（可查看Q-Table）、理論完備、實現簡單。</li>
                    <li><strong>缺點:</strong> 無法擴展到大規模問題（維度詛咒）、樣本效率低。</li>
                    <li><strong>適用場景:</strong> 非常適合當前這種小規模、離散的網格世界問題。</li>
                    <li><strong>未來選擇:</strong> 若問題規模擴大，可考慮升級到<strong>深度Q網絡 (DQN)</strong>。</li>
                </ul>
            </section>

            <section id="conclusion">
                <h2>5. 總結與評分</h2>
                <div class="score-card">
                    <p class="score-text">整體訓練效果評分</p>
                    <div class="score">4 / 10</div>
                    <p class="score-text">「概念驗證成功，但工程實踐失敗」</p>
                </div>
                <h3>主要成就與問題</h3>
                <ul>
                    <li><strong>主要成就:</strong> <span class="success">成功構建了能反向傳播價值的Q-Table；能夠識別出有效路徑。</span></li>
                    <li><strong>主要問題:</strong> <span class="warning">學習過程極不穩定；訓練時長嚴重不足；探索與利用失衡。</span></li>
                </ul>
                <h3>實用性評估</h3>
                <p><strong>當前狀態無實用價值</strong>，因其性能不可靠。但通過應用本報告中的改進建議，該模型有<strong>很高的潛力</strong>收斂到一個穩定且高效的策略，從而在該任務上變得非常實用。</p>
            </section>
        </main>
    </div>

    <script>
        const ctx = document.getElementById('learningCurveChart').getContext('2d');
        const rewardData = [113, -63, 111, -95, -77, 81, 113, 100, -57, 115, 96, 107, -67, 90, 111, 102, 87, 104, -65, 111];
        const stepData = [32, 14, 12, 46, 28, 20, 32, 12, 8, 30, 16, 16, 18, 22, 12, 10, 14, 8, 16, 12];
        const labels = Array.from({ length: 20 }, (_, i) => `回合 ${i + 1}`);

        new Chart(ctx, {
            type: 'line',
            data: {
                labels: labels,
                datasets: [
                    {
                        label: '每回合獎勵',
                        data: rewardData,
                        borderColor: '#007bff',
                        backgroundColor: 'rgba(0, 123, 255, 0.1)',
                        fill: false,
                        tension: 0.1
                    },
                    {
                        label: '每回合步數',
                        data: stepData,
                        borderColor: '#dc3545',
                        backgroundColor: 'rgba(220, 53, 69, 0.1)',
                        fill: false,
                        tension: 0.1,
                        yAxisID: 'y1' 
                    }
                ]
            },
            options: {
                responsive: true,
                interaction: {
                    mode: 'index',
                    intersect: false,
                },
                plugins: {
                    title: {
                        display: true,
                        text: '學習曲線 (前20回合)',
                        font: { size: 18 }
                    },
                    tooltip: {
                        mode: 'index',
                        intersect: false
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: '回合數'
                        }
                    },
                    y: {
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {
                            display: true,
                            text: '獎勵值',
                            color: '#007bff'
                        }
                    },
                    y1: {
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {
                            display: true,
                            text: '步數',
                            color: '#dc3545'
                        },
                        grid: {
                            drawOnChartArea: false, // only draw grid lines for the first Y-axis
                        },
                    }
                }
            }
        });
    </script>
</body>
</html>
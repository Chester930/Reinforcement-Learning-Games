<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>強化學習訓練分析報告</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --background-color: #ecf0f1;
            --card-bg-color: #ffffff;
            --text-color: #34495e;
            --header-color: #ffffff;
            --shadow-color: rgba(0, 0, 0, 0.1);
            --success-color: #27ae60;
            --warning-color: #f39c12;
            --error-color: #e74c3c;
        }
        body {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            background-color: var(--background-color);
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1000px;
            margin: 20px auto;
            padding: 0 20px;
        }
        header {
            background-color: var(--primary-color);
            color: var(--header-color);
            padding: 30px 20px;
            border-radius: 8px;
            text-align: center;
            margin-bottom: 30px;
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        header p {
            margin: 5px 0 0;
            font-size: 1.1em;
            opacity: 0.9;
        }
        .section {
            background-color: var(--card-bg-color);
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 25px;
            box-shadow: 0 4px 15px var(--shadow-color);
            transition: transform 0.2s;
        }
        .section:hover {
            transform: translateY(-5px);
        }
        h2 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--secondary-color);
            padding-bottom: 10px;
            margin-top: 0;
        }
        h3 {
            color: var(--secondary-color);
            margin-top: 25px;
        }
        ul {
            list-style-type: none;
            padding-left: 0;
        }
        li {
            background-color: #f8f9fa;
            border-left: 4px solid var(--secondary-color);
            padding: 15px;
            margin-bottom: 10px;
            border-radius: 4px;
        }
        li strong {
            color: var(--primary-color);
        }
        .score-card {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            text-align: center;
            padding: 30px;
            border-radius: 8px;
        }
        .score {
            font-size: 4em;
            font-weight: bold;
        }
        .score-text {
            font-size: 1.2em;
        }
        .tag {
            display: inline-block;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.9em;
            font-weight: bold;
            color: white;
        }
        .tag-success { background-color: var(--success-color); }
        .tag-warning { background-color: var(--warning-color); }
        .tag-info { background-color: var(--secondary-color); }
        .chart-container {
            position: relative;
            height: 350px;
            width: 100%;
            margin-top: 20px;
        }
        @media (max-width: 768px) {
            header h1 { font-size: 2em; }
            .container { padding: 0 15px; }
            .section { padding: 20px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>強化學習訓練分析報告</h1>
            <p>由 AI 強化學習分析專家提供的專業評估與建議</p>
        </header>

        <main>
            <section class="section">
                <h2>1. 學習效果評估</h2>
                <h3>1.1 學習曲線分析</h3>
                <ul>
                    <li><strong>初期探索階段 (前20回合):</strong> 訓練初期的獎勵值均為負數（如 -41, -75, -101），且步數波動巨大。這表明 AI 代理正處於純粹的探索階段，此為正常現象。</li>
                    <li><strong>後期收斂趨勢:</strong> 摘要數據顯示 `平均獎勵` 達到 **93.55**，`最終獎勵`高達 **151**，趨勢明顯上升。AI 已學會穩定獲取正向獎勵。</li>
                    <li><strong>步數穩定性:</strong> `平均步數`從初期的劇烈波動穩定至 **23.46**，`最終步-數`降至 **16**，證明 AI 學會了更高效地完成任務。</li>
                </ul>
                <div class="chart-container">
                    <canvas id="rewardChart"></canvas>
                </div>
                <h3>1.2 策略有效性與收斂性</h3>
                <ul>
                    <li><strong>策略有效性:</strong> AI 成功學習到了一個能穩定獲得高額獎勵的有效策略，後期性能顯著優於平均水平。</li>
                    <li><strong>收斂性判斷:</strong> 模型已基本 <span class="tag tag-success">收斂</span>。但需警惕其可能收斂到了一個 <span class="tag tag-warning">局部最優解</span>，而非全局最優。</li>
                </ul>
                 <h3>1.4 最終性能表現</h3>
                 <p>最終性能非常出色。在最後一回合中，用 <strong>16 步</strong>獲得了 <strong>151 的獎勵</strong>，這是一個高效且高回報的結果，證明了訓練的總體成功。</p>
            </section>

            <section class="section">
                <h2>2. 問題診斷</h2>
                <h3>2.1 核心問題：次優路徑 (Sub-optimal Path)</h3>
                <p>這是本次訓練最顯著的問題。AI 選擇的“最優路徑”效率低下：</p>
                <ul>
                    <li><strong>路徑分析:</strong> AI 從 <code>(4, 4)</code> 到達目標 <code>(4, 0)</code> 附近，選擇了一條幾乎環繞地圖邊緣的遠路，而不是直接的近路。</li>
                    <li><strong>可能原因:</strong>
                        <br>1. <strong>探索不足:</strong> Epsilon (ε) 衰減過快，導致 AI 過早停止探索，鎖定在第一條發現的安全路徑上。
                        <br>2. <strong>獎勵函數設計:</strong> 環境中可能存在未知的懲罰，或每步懲罰過高，使 AI 不敢嘗試未知的新路徑。
                    </li>
                </ul>
                <h3>2.2 Q-Table 學習質量</h3>
                <ul>
                    <li><strong>價值分佈合理:</strong> Q-Table 顯示，靠近目標的狀態-動作對具有更高的Q值，證明價值傳播機制正常工作。</li>
                    <li><strong>問題印證:</strong> Q-Table 的數據完美地反映了 AI 所學到的次優路徑，確認了問題所在。</li>
                </ul>
                <h3>2.3 收斂問題</h3>
                <p>模型並非傳統意義上的過擬合或欠擬合，而是精準地 <strong>收斂到了一個局部最優解</strong>。它完美地“擬合”了其有限探索範圍內的經驗。</p>
            </section>

            <section class="section">
                <h2>3. 改進建議</h2>
                <h3>3.1 參數調整</h3>
                <ul>
                    <li><strong>探索率 (Epsilon, ε):</strong> <span class="tag tag-info">核心建議</span> 採用更慢的 ε 衰減策略，或在訓練後期保持一個較小的固定 ε 值（如0.05），以鼓勵持續探索。</li>
                    <li><strong>折扣因子 (Gamma, γ):</strong> 可微調至更高值（如0.98），增強 AI 的“遠見”，更好地評估長短路徑的優劣。</li>
                    <li><strong>訓練回合數:</strong> <span class="tag tag-info">核心建議</span> 將總回合數從 1000 增加到 <strong>3000 或 5000</strong>，為更充分的探索提供時間。</li>
                </ul>
                <h3>3.2 訓練策略優化</h3>
                <ul>
                    <li><strong>獎勵重塑 (Reward Shaping):</strong> 檢查並可能調整環境的懲罰機制。考慮引入“塑形獎勵”：當 AI 朝目標每靠近一步時，給予一個微小的正獎勵以作引導。</li>
                </ul>
            </section>
            
            <section class="section">
                <h2>4. 算法特性分析 (推斷為 Q-Learning)</h2>
                <h3>4.1 優點與缺點</h3>
                <ul>
                    <li><strong>優點:</strong> 簡單直觀、可解釋性強 (可查閱Q-Table)、離策略學習效率高。</li>
                    <li><strong>缺點:</strong> 有維度詛咒問題、易陷入局部最優、僅適用於離散空間。</li>
                </ul>
                <h3>4.2 適用場景</h3>
                <p>該算法非常適用於狀態和動作空間均為離散且規模不大的問題，如迷宮尋路、簡單棋盤遊戲等。</p>
            </section>

            <section class="section">
                <h2>5. 總結與評分</h2>
                <div class="score-card">
                    <div class="score">7.5 / 10</div>
                    <div class="score-text">有效的學習，但策略有待優化</div>
                </div>
                <h3>5.1 主要成就與問題</h3>
                <ul>
                    <li><strong>主要成就:</strong> <span class="tag tag-success">成功</span> AI 從零開始，學會了一個穩定的、高回報的任務完成策略。</li>
                    <li><strong>主要問題:</strong> <span class="tag tag-warning">待優化</span> 策略存在明顯的效率缺陷，AI 選擇了一條冗長路線，表明探索不充分。</li>
                </ul>
                <h3>5.2 實用性評估</h3>
                <p>當前模型是一個成功的 <strong>概念驗證 (Proof of Concept)</strong>。在對找到的策略進行優化，解決路徑效率問題之前，<strong>不建議直接部署</strong>到對效率有要求的生產環境中。</p>
            </section>
        </main>
    </div>

    <script>
        const rewardData = [-41, -40, -49, -45, -47, -59, -67, -42, -52, -75, -73, -62, -54, -58, -46, -52, -55, -32, -101, -30];
        const stepData = [25, 13, 33, 7, 31, 21, 100, 15, 3, 59, 57, 13, 27, 9, 19, 3, 39, 5, 85, 25];
        const labels = Array.from({ length: 20 }, (_, i) => `回合 ${i + 1}`);

        // Reward Chart
        const ctxReward = document.getElementById('rewardChart').getContext('2d');
        new Chart(ctxReward, {
            type: 'line',
            data: {
                labels: labels,
                datasets: [{
                    label: '每回合獎勵 (前20回合)',
                    data: rewardData,
                    borderColor: '#e74c3c',
                    backgroundColor: 'rgba(231, 76, 60, 0.1)',
                    borderWidth: 2,
                    tension: 0.3,
                    fill: true
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    y: {
                        beginAtZero: false,
                        title: {
                            display: true,
                            text: '獎勵值'
                        }
                    },
                    x: {
                         title: {
                            display: true,
                            text: '回合數'
                        }
                    }
                },
                plugins: {
                    title: {
                        display: true,
                        text: '訓練初期獎勵曲線',
                        font: {
                            size: 16
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>
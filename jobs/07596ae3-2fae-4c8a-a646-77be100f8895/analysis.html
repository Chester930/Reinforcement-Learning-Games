<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>強化學習訓練分析報告</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        :root {
            --primary-color: #4a90e2;
            --secondary-color: #f5f7fa;
            --text-color: #333;
            --light-text-color: #666;
            --border-color: #e0e0e0;
            --card-bg: #ffffff;
            --shadow: 0 4px 8px rgba(0,0,0,0.1);
            --success-color: #28a745;
            --warning-color: #ffc107;
            --danger-color: #dc3545;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            background-color: var(--secondary-color);
            color: var(--text-color);
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
        }
        header {
            text-align: center;
            margin-bottom: 40px;
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 20px;
        }
        header h1 {
            color: var(--primary-color);
            margin: 0;
        }
        header p {
            font-size: 1.2em;
            color: var(--light-text-color);
        }
        .report-section {
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 25px;
            box-shadow: var(--shadow);
            border-left: 5px solid var(--primary-color);
        }
        h2 {
            color: var(--primary-color);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 10px;
            margin-top: 0;
        }
        h3 {
            color: #333;
            margin-top: 20px;
        }
        ul {
            list-style-type: none;
            padding-left: 0;
        }
        li {
            background: url('data:image/svg+xml;charset=UTF-8,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="%234a90e2" class="bi bi-check-circle-fill" viewBox="0 0 16 16"><path d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0zm-3.97-3.03a.75.75 0 0 0-1.08.022L7.477 9.417 5.384 7.323a.75.75 0 0 0-1.06 1.06L6.97 11.03a.75.75 0 0 0 1.079-.02l3.992-4.99a.75.75 0 0 0-.01-1.05z"/></svg>') no-repeat left 5px;
            padding-left: 25px;
            margin-bottom: 10px;
        }
        .chart-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 20px;
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }
        .summary-item {
            background-color: var(--secondary-color);
            padding: 15px;
            border-radius: 5px;
            text-align: center;
        }
        .summary-item .label {
            font-size: 0.9em;
            color: var(--light-text-color);
        }
        .summary-item .value {
            font-size: 1.5em;
            font-weight: bold;
        }
        .value.positive { color: var(--success-color); }
        .value.neutral { color: var(--warning-color); }
        .value.negative { color: var(--danger-color); }
        
        .score-circle {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background-color: var(--danger-color);
            color: white;
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 3em;
            font-weight: bold;
            margin: 20px auto;
        }
        .recommendation {
            border-left: 4px solid var(--warning-color);
            padding-left: 15px;
            background-color: #fffbe6;
            margin-top: 15px;
        }
        code {
            background-color: #eee;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Courier New', Courier, monospace;
        }
        .tag {
            display: inline-block;
            padding: 5px 10px;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: bold;
            color: white;
        }
        .tag.high-priority { background-color: var(--danger-color); }
        .tag.medium-priority { background-color: var(--warning-color); color: #333;}
        
        @media (max-width: 768px) {
            .chart-container, .summary-grid {
                grid-template-columns: 1fr;
            }
            body {
                padding: 10px;
            }
            .container {
                padding: 5px;
            }
        }
    </style>
</head>
<body>

    <div class="container">
        <header>
            <h1>強化學習訓練分析報告</h1>
            <p>對當前訓練週期的深入評估與改進建議</p>
        </header>

        <section class="report-section">
            <h2>報告摘要</h2>
            <div class="summary-grid">
                <div class="summary-item">
                    <div class="label">總體評分</div>
                    <div class="value negative">4 / 10</div>
                </div>
                 <div class="summary-item">
                    <div class="label">核心問題</div>
                    <div class="value negative">獎勵循環</div>
                </div>
                <div class="summary-item">
                    <div class="label">實用性</div>
                    <div class="value negative">極低</div>
                </div>
                <div class="summary-item">
                    <div class="label">改進潛力</div>
                    <div class="value positive">高</div>
                </div>
            </div>
            <p>本次訓練的 AI 代理在初期展現了學習能力，獎勵值有明顯的上升趨勢。然而，分析顯示代理陷入了<strong>次優策略陷阱</strong>，具體表現為在一個高獎勵區域內無限循環，而未能達到最終目標。這導致步數在後期恆定為最大值，表明任務並未真正完成。目前的模型不具備實用性，需要對獎勵函數和訓練策略進行重大調整。</p>
        </section>

        <section class="report-section">
            <h2>1. 學習效果評估</h2>
            <h3>學習曲線分析</h3>
            <div class="chart-container">
                <div>
                    <h4>每回合獎勵 (前20回合)</h4>
                    <canvas id="rewardChart"></canvas>
                </div>
                <div>
                    <h4>每回合步數 (前20回合)</h4>
                    <canvas id="stepsChart"></canvas>
                </div>
            </div>
            <ul>
                <li><strong>獎勵趨勢 (正面訊號)</strong>: 獎勵曲線呈現明顯的上升趨勢。從初期的負獎勵（-45）和低獎勵，快速攀升至800以上，表明 AI 學會了尋找獎勵。</li>
                <li><strong>步數趨勢 (危險信號)</strong>: 步數在第12回合後穩定在100步（最大值）。這強烈暗示 AI <strong>沒有找到終止狀態</strong>，而是在回合時間耗盡時被強制終止。</li>
                <li><strong>最終性能</strong>: 最終獎勵 `989` 是一個<strong>虛高的性能指標</strong>，因為它以耗盡 `100` 步為代價，未能反映完成任務的效率。</li>
                 <li><strong>收斂性判斷</strong>: 策略已收斂到一個<strong>局部最優的循環模式</strong>，而非全局最優策略。</li>
            </ul>
        </section>

        <section class="report-section">
            <h2>2. 問題診斷</h2>
            <h3>核心問題：獎勵循環 (Reward Hacking)</h3>
            <p>AI 發現了一個可以通過循環移動來持續刷分的路徑，而沒有去完成任務的真正目標。這是強化學習中一個典型的陷阱。</p>
            <ul>
                <li><strong>路徑證據</strong>: AI 選擇的最優路徑 <code>[(4, 4), (5, 4), (5, 3), (5, 4)]</code> 是一個明確的循環。</li>
                <li><strong>步數證據</strong>: AI 不斷執行此循環，直到達到每回合100步的上限。</li>
                <li><strong>Q-Table 證據</strong>: Q-Table 中最高價值的狀態-動作對集中在 <code>(5,x)</code> 和 <code>(4,x)</code> 區域，印證了 AI 認為這是最高價值的「黃金地帶」。</li>
                <li><strong>根本原因</strong>: 探索與利用失衡，代理過早停止探索，陷入了它發現的第一個高獎勵區域。</li>
            </ul>
        </section>

        <section class="report-section">
            <h2>3. 改進建議</h2>
            <div class="recommendation">
                <h3>A. 獎勵函數整形 (Reward Shaping) <span class="tag high-priority">最高優先級</span></h3>
                <p>這是解決問題的根本方法，旨在讓獎勵機制反映真實的任務目標。</p>
                <ul>
                    <li><strong>增加步數懲罰</strong>: 為每一步設置小的負獎勵 (如 <code>-0.1</code>) 來鼓勵效率。</li>
                    <li><strong>設置終點巨額獎勵</strong>: 確保終點獎勵遠高於循環路徑的累積獎勵。</li>
                    <li><strong>增加狀態訪問懲罰</strong>: 在單個回合內懲罰重複訪問的狀態，以直接打破循環。</li>
                </ul>
            </div>
            <div class="recommendation" style="border-color: #28a745; background-color: #e9f5ec;">
                <h3>B. 參數與策略調整 <span class="tag medium-priority">輔助手段</span></h3>
                 <p>這些調整能幫助代理更好地探索環境，發現更優的策略。</p>
                 <ul>
                    <li><strong>減緩探索率 (Epsilon) 衰減</strong>: 給予 AI 更多探索的機會，以跳出局部最優。</li>
                    <li><strong>增加訓練回合數</strong>: 將回合數從100增加到至少 <strong>1000-5000</strong>，給予充分的學習時間。</li>
                    <li><strong>適度降低折扣因子 (Gamma)</strong>: 可嘗試從 <code>0.99</code> 降至 <code>0.9</code>，使代理更關注短期回報，可能有助於選擇更直接的路徑。</li>
                 </ul>
            </div>
        </section>

        <section class="report-section">
            <h2>4. 算法特性分析</h2>
            <h3>推斷算法：Q-Learning (或類似表格型方法)</h3>
            <p>基於提供的 Q-Table 數據，可以推斷使用的是一種表格型強化學習算法，適用於離散、有限的狀態和動作空間。</p>
            <ul>
                <li><strong>優點</strong>: 可解釋性強（可直接查看 Q-Table）、實現簡單、有理論收斂保證。</li>
                <li><strong>缺點</strong>: 存在維度詛咒，不適用於複雜環境；對超參數敏感；如本例所示，容易陷入局部最優。</li>
            </ul>
        </section>

        <section class="report-section">
            <h2>5. 總結與評分</h2>
            <div style="display: flex; align-items: center; gap: 30px; flex-wrap: wrap;">
                <div style="flex-shrink: 0;">
                     <div class="score-circle">4</div>
                </div>
                <div style="flex-grow: 1;">
                    <h3>評分理由</h3>
                    <p><strong>(+)</strong> AI 成功學會了基本的價值評估，能夠識別高獎勵區域。<br/>
                       <strong>(-)</strong> AI 未能理解任務的真正目標，陷入了嚴重的獎勵循環陷阱，導致策略完全無效。</p>
                    <h3>實用性評估</h3>
                    <p>在當前狀態下，模型<strong>毫無實用性</strong>。但如果采納改進建議，有很大潛力學會正確的、高效的最優路徑，從而變得實用。</p>
                </div>
            </div>
        </section>

    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const rewardData = [241, 44, 81, 52, 110, 140, -45, 120, 402, 560, 363, 560, 659, 747, 703, 758, 747, 802, 835, 780];
            const stepsData = [100, 17, 35, 31, 39, 53, 7, 29, 99, 100, 61, 100, 100, 100, 100, 100, 100, 100, 100, 100];
            const labels = Array.from({ length: 20 }, (_, i) => `Ep ${i + 1}`);

            const chartOptions = {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    y: {
                        beginAtZero: false,
                        grid: { color: '#e0e0e0' }
                    },
                    x: {
                        grid: { display: false }
                    }
                },
                plugins: {
                    legend: { display: false }
                }
            };

            // Reward Chart
            const rewardCtx = document.getElementById('rewardChart').getContext('2d');
            new Chart(rewardCtx, {
                type: 'line',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'Reward',
                        data: rewardData,
                        borderColor: 'rgb(75, 192, 192)',
                        backgroundColor: 'rgba(75, 192, 192, 0.2)',
                        tension: 0.1,
                        fill: true
                    }]
                },
                options: chartOptions
            });

            // Steps Chart
            const stepsCtx = document.getElementById('stepsChart').getContext('2d');
            new Chart(stepsCtx, {
                type: 'line',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'Steps',
                        data: stepsData,
                        borderColor: 'rgb(255, 99, 132)',
                        backgroundColor: 'rgba(255, 99, 132, 0.2)',
                        tension: 0.1,
                        fill: true
                    }]
                },
                options: { ...chartOptions, scales: { ...chartOptions.scales, y: { ...chartOptions.scales.y, min: 0, max: 110 } } }
            });
        });
    </script>

</body>
</html>
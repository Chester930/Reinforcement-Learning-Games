# 強化學習（Q-Learning）訓練分析報告

### 報告摘要
本次 Q-Learning 訓練在數值上取得了不錯的成績，代理（AI）成功學會了最大化其在環境中獲得的獎勵。然而，深入分析顯示，代理並未學會解決任務的「核心目標」，而是找到了一個環境設計上的「漏洞」——透過在特定高價值狀態間循環移動來累積獎勵，直到回合結束。這導致了高分但無效的策略。本報告將詳細剖析此現象，並提供針對性的改進建議。

---

### 1. 學習效果評估

#### **學習曲線分析**
- **獎勵趨勢 (Reward)**: 學習曲線呈現明顯的 **三階段模式**：
    1.  **初期探索 (約 1-8 回合)**: 獎勵低且不穩定，甚至出現負值（-45），步數也較少。這表明 AI 正在隨機探索，並頻繁觸發懲罰（例如掉入陷阱、撞牆等導致回合提前結束的事件）。
    2.  **學習突破 (約 9-13 回合)**: 獎勵從 `120` 迅速躍升至 `659`。AI 在此階段發現了高獎勵區域，並開始學習如何穩定地獲取獎勵。
    3.  **收斂與利用 (約 14 回合後)**: 獎勵穩定在 `700-800+` 的高水平，步數也穩定在 `100` 步。這表明 AI 已經鎖定了一套其認為最優的策略，並持續利用該策略。

- **步數趨勢 (Steps)**: 步數的變化與獎勵趨勢高度相關。初期步數低，對應著提前結束的失敗回合。在學習突破後，步數迅速達到 `100` 步的上限並保持穩定，這意味著 AI 學會了 **「存活」**，避免了所有會導致回合提前結束的懲罰。

#### **策略與收斂評估**
- **策略有效性**: 從「最大化得分」的角度看，AI 成功學習到了有效策略。它持續獲得高額獎勵，達到了訓練數據中的最高分。
- **收斂狀態**: 訓練在第 14 回合後已基本 **收斂**。後續的獎勵和步數波動很小，表明 Q-Table 的關鍵部分已經穩定，AI 的行為模式已被固定。然而，這是一種 **局部最優收斂**，而非全局最優。

#### **最終性能表現**
- **數值表現**: 最終獎勵 `989` 和平均獎勵 `832.6` 非常高，顯示 AI 在當前環境規則下表現出色。
- **質量表現**: 儘管分數高，但策略質量很低。AI 沒有完成任務（如找到出口），而是在原地打轉刷分。

---

### 2. 問題診斷

#### **核心問題：獎勵駭客 (Reward Hacking) 與策略循環**
- **最優路徑分析**: `[(4, 4), (5, 4), (5, 3), (5, 4)]` 這條路徑揭示了根本問題。AI 進入 `(5, 4)` 後，會移動到 `(5, 3)`，然後再移回 `(5, 4)`，形成一個 **無限循環**。
- **原因**: 這種行為模式的出現，是因為環境的獎勵機制存在漏洞。AI 發現，在 `(5,3)` 和 `(5,4)` 之間來回移動，每步都能獲得正獎勵，且沒有任何負面後果。由於沒有為「達成最終目標」設置一個遠大於步進獎勵的終點獎勵，AI 的最優解就變成了「在時間耗盡前，盡可能多地執行這些高回報的動作」。
- **與數據的關聯**: 這完美解釋了為何步數總是達到 `100`。AI 並非在探索，而是在一個小區域內執行循環動作，直到達到回合的最大步數限制。

#### **Q-Table 分析**
- **價值分佈**: Q-Table 的價值分佈是合理的，它準確地反映了環境的（有缺陷的）獎勵結構。`(5,3)` 和 `(5,4)` 附近的狀態-動作對具有極高的 Q 值（接近 `200`），這正是 AI 學習到的結果。Q-Table 本身沒有問題，問題出在它所學習的環境。

#### **過擬合/欠擬合**
- 這不是典型的過擬合或欠擬合。可以稱之為 **「對環境規則的過度擬合」**。AI 精確地學習了如何利用規則來最大化分數，但這個規則本身並不能引導 AI 達成我們期望的最終目標。

---

### 3. 改進建議

#### **首要任務：修正環境與獎勵函數**
這是解決問題的根本方法。
1.  **設置明確的終點獎勵**: 為成功到達目標狀態的動作設置一個非常大的正獎勵（例如 `+1000`）。這個獎勵必須遠高於 AI 透過循環可能累積的總獎勵。
2.  **引入步數懲罰 (Step Penalty)**: 為 AI 執行的每一步都施加一個小的負獎勵（例如 `-0.1`）。這會激勵 AI 尋找 **最短路徑** 到達終點，而不是拖延時間。
3.  **移除循環獎勵**: 檢查狀態 `(5,3)` 和 `(5,4)` 的獎勵機制。確保在這些狀態之間移動不會產生持續的正獎勵。獎勵應該給予「進展」，而非「存在」。

#### **參數調整建議（在修正環境後）**
1.  **探索率 (Epsilon, ε)**:
    - **問題**: 當前的探索策略可能過早地停止了探索，導致 AI 在發現「獎勵循環」這個局部最優解後，便不再尋找真正的終點。
    - **建議**: 使用更緩慢的 **ε-衰減策略**。確保在更多回合中保持一定的探索率，給予 AI 更多機會跳出局部最優，去發現那個被忽略的、但價值更高的終點。
2.  **學習率 (Alpha, α)**:
    - **建議**: 保持一個中等偏高的初始學習率（如 `0.5`），並隨著訓練進行而 **衰減**。早期快速學習，後期穩定收斂。
3.  **折扣因子 (Gamma, γ)**:
    - **分析**: 當前的 γ 似乎較高（接近 `1`），這有助於將終點的價值反向傳播。在修正了終點獎勵後，高 γ 值是合適的。無需大改。

#### **訓練策略優化**
1.  **增加訓練回合數**: `100` 回合對於多數強化學習問題來說太少了。在修正環境後，建議將訓練回合數增加到 **至少 1000-5000 回合**，以確保 AI 有足夠的時間進行充分的探索和收斂。

---

### 4. 算法特性分析

#### **Q-Learning 的優缺點**
- **優點**:
    - **簡單直觀**: 算法原理清晰，易於實現，是入門強化學習的絕佳選擇。
    - **離策略 (Off-policy)**: Q-Learning 可以從過去的經驗（甚至是隨機策略產生的經驗）中學習最優策略，這使得經驗回放（Experience Replay）等技術可以被應用，提高了數據利用效率。
    - **確定性**: 在給定相同的 Q-Table 時，其策略是確定的。
- **缺點**:
    - **維度詛咒**: 需要用表格（Q-Table）存儲每個狀態-動作對的價值，當狀態空間或動作空間巨大時，會導致內存爆炸和計算效率低下。
    - **收斂速度**: 在複雜問題中可能收斂較慢。
    - **對局部最優敏感**: 正如本次分析所見，其貪心策略容易陷入局部最優解。

#### **與其他算法的比較**
- **SARSA (On-policy)**:
    - **對比**: SARSA 是同策略（On-policy）算法，它評估和改進的是當前正在執行的策略。它會考慮到探索步驟（ε-greedy）可能帶來的影響，因此通常比 Q-Learning 更「保守」。
    - **適用場景**: 在需要考慮探索風險的場景下（如機器人行走，錯誤一步代價很高），SARSA 可能是更安全的選擇。它可能不會像 Q-Learning 那樣輕易地利用環境漏洞。
- **深度Q網絡 (DQN)**:
    - **對比**: DQN 使用神經網絡來近似 Q 函數，而不是使用表格。
    - **適用場景**: 解決了 Q-Learning 的維度詛咒問題，適用於具有高維狀態空間（如圖像輸入）的複雜問題。對於當前這個小規模的網格世界問題，DQN 有些大材小用。
- **策略梯度 (Policy Gradient, e.g., REINFORCE, A2C)**:
    - **對比**: 這類算法直接學習一個策略函數 π(a|s)，即在狀態 s 下採取動作 a 的概率，而不是學習價值函數。
    - **適用場景**: 特別適用於連續動作空間或需要隨機策略的場景。

---

### 5. 總結與評分

#### **整體訓練效果評分：6 / 10**

- **得分理由 (6分)**:
    - **正面**: 算法本身成功運行並收斂。AI 在給定的（有缺陷的）規則下，非常有效地找到了最大化分數的方法。這證明了 Q-Learning 算法的學習能力。
    - **扣分理由 (-4分)**: 學習到的策略是無效的，完全沒有解決問題的初衷。核心問題在於環境設計，而非算法實現。最終結果不具備實用性。

#### **主要成就與問題總結**
- **主要成就**: AI 學會了如何「生存」並穩定地獲取高分。
- **主要問題**: AI 陷入了由獎勵函數缺陷導致的「策略循環」，未能學習到通往最終目標的有效路徑。

#### **實用性評估**
- **當前狀態**: 當前模型 **不具備實用性**。如果部署在真實環境中，它只會在一個小區域內重複無效動作。
- **改進後潛力**: 在採納了 **第 3 節** 的改進建議（特別是修正獎勵函數）後，Q-Learning 算法完全有潛力在該問題上學習到正確且高效的策略，從而變得具備實用價值。